{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "hfskCiZOupgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LM_MODEL = \"gemma-3-270m-it\"\n",
        "HF_MODEL_REPO = \"mayurmadnani/gemma-3-270m-microfables\""
      ],
      "metadata": {
        "id": "Smvan32oXw0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "8nQnRb8bdTUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "zCSHjP2iG92d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = f\"unsloth/{LM_MODEL}\",\n",
        "    max_seq_length = max_seq_length, # for long context\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False,\n",
        ")"
      ],
      "metadata": {
        "id": "1K6hgVXfHGCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464,
          "referenced_widgets": [
            "ad666dfd3b0040a8b85c82abb0dc7164",
            "a117067740a942daaf064fa85b6ff860",
            "1556f10830d344648decbb4309b55b9b",
            "5649e51858144559a8525932ce275ac8",
            "4793c239640f42789995f23bda08f809",
            "34e4e5d1a12448e78849b0f2ac76bd43",
            "f83f760b9fca41ec89ab8a27f8478692",
            "4f4da1596eae4f3594677122bce86f54",
            "516a2f32fff545d68f96fe8932f60ca4",
            "79674b65fd2041668525e754466e7331",
            "2ab387bc751347519815682098c93aec",
            "e61ee4a36471476193fdcedbb193d8bb",
            "420100b1685d4dcd85f0e561bb675cad",
            "a57ac3a4d93b460c86fc7d604d30b3bf",
            "4144e007fd754fd387a816b0a899835c",
            "eb85567260ad426db8aa24dc4aec2c73",
            "dd65f1a0c58148189fe192d0c465ec91",
            "deadd04e1843479bb9a5b8d74217cd48",
            "1b4d024bbc484557ace46da26d70b51a",
            "460c5a0ae43d44d4a3159b25826dd1f4",
            "71e9469e6b3f4d9dac0fd58da0be6722",
            "ab0c9fc79a714f7a8dac2290cc3a8b1f",
            "1826c0f56c42462c96cfe512613ec169",
            "53c7ae68b8a645dbb9c81d2def8f2e38",
            "ed658a5173184d458faf01dfe1bb91bb",
            "6c86c4c3f73a4c3a9ae3df30c6afa502",
            "b1b1e1eef93e454a80034205cad073ce",
            "2bc68d34b479473180a532ddaa58cd87",
            "bc25787fb1a6430e9c6b0bf24410d3f3",
            "684423b1fb79457d8e04bb7a2b8accf5",
            "948187929b3e4ff0acf8eeff2e8852b4",
            "9dc2e82f407d45f9b3626a5992671f3c",
            "e3c16a229eb248d7b6aa61fa8c936075",
            "b13c8bb8357b4a6d93c912b8a6bcab40",
            "d4ba3a0d2ca3424f8fd016fd2544dcd6",
            "620ceb3a40d744f5bf6822dc25b5bb4b",
            "8f71a32bb5dc45f6bca644be768802c3",
            "00cb56b6a72347dda51bb8e68597c382",
            "655e3c4ce4d4483eb43bf69c5d9c3494",
            "88013eb584214757be8fbb973dd6678e",
            "6592b50688ab41b09d6570f754daadf4",
            "4e5844a449b149a390d4475f5e5ead91",
            "9fa5d6d8d6df4e2a962a486ebe536961",
            "8fde0d8a0c9b40a0b9607d485f33aba3",
            "be2cfaf2313c4d9499c87540552c727f",
            "99a8696254b24e3fbf21278a4a6252d2",
            "822610a1dd57475598bea7d3e80e2a57",
            "e7a4aee08ea448b9836935bc1b61cd95",
            "ec7cd561363f4b34ba7096fc86c18764",
            "ab9322171cef450282c622ed928f3077",
            "a2e4085dfc0a4b859869ffffd9fbe70c",
            "e0c239af2b3a4a1f827443b6c7c1eda4",
            "173526df4ea54f3c87f49a5e960f4a36",
            "51e86bf813004b5eb6ac03cb45af40a2",
            "207c84bbc05947f485a89ff4bc3460c2",
            "926a4a3567174a96bf839dcee627441d",
            "25fd543064104c0ca9bebb32eed5ea75",
            "4be688fd5bbf4813a5f26f9f59bc980b",
            "5e20e277775541bab3bc97239d307fbb",
            "dbc90a0bf1e14e2f95d29b3e6a737fca",
            "70c9ffd4c4da41a5824f2433644671dd",
            "7d25aceafb634d03bcf01ad7d93322f3",
            "a456075b1e594870ba703f0323e96be5",
            "19d57467ac094e729b2b8d624386fcbb",
            "eafc4b7ed9834bf0b1eca8f96492f1d5",
            "f785f101f7ba4c269ecfbf66406cb039",
            "5d426033eb674defb991c7f35def0d74",
            "04b1e38a5bb84e31bd5706f81ab67396",
            "a40e7316cb264e4e98744e6dc3bd6d86",
            "24c4137140bf48c3a8121e1744080c81",
            "c4e036302f984053a13fcb6375644c3f",
            "cfc0c4787b264e23add8d4190133f3d2",
            "3cf515c42ea24c3f81ddee29b136bef6",
            "45a99aa460d94be7b6001368c948ae6a",
            "473a1152487d48a3a06b73bda9ed8d82",
            "b524fb54be7e4ff2a25bc565a10d6897",
            "dfc156be8f5b4bab8f3b51064745200c",
            "88894aea4d094997af367e9970d1ccc3",
            "550fbe4b647d4e6c996e1d09d385d016",
            "3fc908c477cf4fb7b3fa997fb663e36e",
            "c5de720ff4be4111a6f1a5f2563bde9d",
            "8a1b3c7f44984f72a26fc79f9ebb8284",
            "ff51a5c22d174a66af59aa209723d89f",
            "748264746adb4241acb8abb93eb7b63a",
            "81fc0aa5d28a4f44bbfa5c00c0de0b5c",
            "7f1ceb7b3b474da09e0b1e2bc1d84f75",
            "30d87db7ec914531898e1e2fcaa335cd",
            "67d6ae2637e3460bb769ae6a7d7d9e63"
          ]
        },
        "outputId": "646d9a30-d912-4b2a-fb82-66463dfaff0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.10.1: Fast Gemma3 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad666dfd3b0040a8b85c82abb0dc7164"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e61ee4a36471476193fdcedbb193d8bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1826c0f56c42462c96cfe512613ec169"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b13c8bb8357b4a6d93c912b8a6bcab40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be2cfaf2313c4d9499c87540552c727f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "926a4a3567174a96bf839dcee627441d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d426033eb674defb991c7f35def0d74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88894aea4d094997af367e9970d1ccc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "uc2j-Dp7HbYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a93d53-c665-43cd-a79b-fa7763e0490d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma3ForCausalLM(\n",
              "  (model): Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-17): 18 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{LM_MODEL}-base\")"
      ],
      "metadata": {
        "id": "Uv5a4vD7_20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # rank stabilized LoRA\n",
        "    loftq_config = None, # LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "IUz0HdSsIN7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c50dc9c-3986-4f4c-bd31-9e5d3daf4b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "7J97k2vWHWFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7321d3-4909-4c5f-9a25-61c1a07f0364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma3ForCausalLM(\n",
              "      (model): Gemma3TextModel(\n",
              "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x Gemma3DecoderLayer(\n",
              "            (self_attn): Gemma3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1024, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=640, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "            )\n",
              "            (mlp): Gemma3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=640, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (rotary_emb): Gemma3RotaryEmbedding()\n",
              "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "p6qi4wjb6O1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma3\",\n",
        ")"
      ],
      "metadata": {
        "id": "zFXBy9IiIYEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.get_chat_template())"
      ],
      "metadata": {
        "id": "HloPxqFXFKgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97e4ea4-bfe4-4375-aa6a-4614d06a9ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{ '<start_of_turn>model\n",
            "' }}\n",
            "{%- endif -%}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files= \"stories_dataset.jsonl\")"
      ],
      "metadata": {
        "id": "mJ9tAoBkIYfB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "99816750bcd94c868066ca8d11289498",
            "f022caa9f63146c983b1ffbc5d2f9a30",
            "7d19a30db8934fe28aad8aba2160fb0a",
            "3a65362ecb7344c29bca80e950ec3d3e",
            "a9bd410f09574438b998dd3632df2b60",
            "2d0b4f872d06440db61b74b29e355b0a",
            "a3518eb12fc84444ac551a52486fb2bd",
            "8e2b818d3b074988aa511b18fadd5480",
            "bc9368227a1b458a8dd87f71286466e9",
            "ad38e8b13e0e45df85060492ba38006a",
            "af402be42e4f480db7cb574e29557640"
          ]
        },
        "outputId": "24752fe2-a5a1-4cf7-f1a5-805dd14ad943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99816750bcd94c868066ca8d11289498"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_chatml(example):\n",
        "    return {\n",
        "        \"conversations\": [\n",
        "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(\n",
        "    convert_to_chatml\n",
        ")"
      ],
      "metadata": {
        "id": "7wKj2TiJIZii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e1266b799af74e4cbcc01b8589e5f7ed",
            "6407cb6137b1477599c4db495ac94e6f",
            "58d98885540b48409c26904d49559ae5",
            "c7a7495dceab4f41ba98a069a208b4e4",
            "b46018b3a3334c7abacc12e15502f99a",
            "b3aec516570142e6b933a61f122b3ff4",
            "8d77b3fe854545249875a1635e696d29",
            "36af72091bf54385bb41b7c67795feb0",
            "9bb8657becb2496286ffc7f10845b86d",
            "5196cc38394048dc8a347fa0a3ee477b",
            "b8e67b95fa8d42178ec5fe381e6a3d37"
          ]
        },
        "outputId": "3408e7fb-9244-41b6-a063-bda2e49a718e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1266b799af74e4cbcc01b8589e5f7ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][10]"
      ],
      "metadata": {
        "id": "TRR9vXgQIbX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1463a5c-ff1d-41b5-b5d7-037969143227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              " 'response': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              " 'conversations': [{'content': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ],
      "metadata": {
        "id": "EdJZCZUNIc2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "daf63396fdef4a55a8ceba1303d9cf22",
            "b1d9f54e05234e03b393e82858219104",
            "64443ea137a344208cffdef6559e0fee",
            "f7ddfc78234e4abeae791101278ec7cf",
            "8064df42832e425aa47455e2235e8f5c",
            "d4a8c89a54a34c108138414ab73c315a",
            "82b164b286034b4d858a7cb4f5855257",
            "1152248e83444fcd93a1fb2e2570c259",
            "03b1869ffbdd4b699e61544161d479a3",
            "0e02b9fde78f4e65bc717c29b46e093d",
            "4c634c81c6674d889a078f9d5f04e52b"
          ]
        },
        "outputId": "01d03118-a33f-4d19-e52f-ad6ed2a5e397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daf63396fdef4a55a8ceba1303d9cf22"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][10]['text'])"
      ],
      "metadata": {
        "id": "qHRdqu53IerA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a450643b-ff74-446a-a786-b059abee10b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "Write a story about a turtle who wins a race against a rabbit.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(f\"{LM_MODEL}-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dDT2Tr9AAP9",
        "outputId": "b5150a51-c6f4-401c-ab86-8c51f3a77376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gemma-3-270m-it-base/tokenizer_config.json',\n",
              " 'gemma-3-270m-it-base/special_tokens_map.json',\n",
              " 'gemma-3-270m-it-base/chat_template.jinja',\n",
              " 'gemma-3-270m-it-base/tokenizer.model',\n",
              " 'gemma-3-270m-it-base/added_tokens.json',\n",
              " 'gemma-3-270m-it-base/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune"
      ],
      "metadata": {
        "id": "4odXsSP3dj3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 10,\n",
        "        # max_steps = 50,\n",
        "        learning_rate = 5e-5,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "zdl4gvrBIfuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "bb902e25617748b4b0da186fb61e7cda",
            "ccaeba49f347451c991011adea91b5eb",
            "9ccae369fc9f473ea6e5e8962453d4ac",
            "057b680f888f45f49756689303c1549a",
            "62881f25e3eb4bb89cda05ba84ebb501",
            "56e132b35bfe4cf2be07220579696f47",
            "d1b6a1fa399f4c9b819e995d877e2466",
            "f81f241a24c44933a70b420b599dc993",
            "c607404db4ff45d6a31b565b568e5248",
            "4036bd33071840099230cc4b94601731",
            "64881afd38304a82af7f1edbd025d090"
          ]
        },
        "outputId": "384a52b3-e783-4088-a961-2305b888718b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb902e25617748b4b0da186fb61e7cda"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma4q6cozHGEF",
        "outputId": "039f68c9-3ff9-47fb-9729-236fedf0b9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              " 'response': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              " 'conversations': [{'content': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              "   'role': 'assistant'}],\n",
              " 'text': '<start_of_turn>user\\nWrite a story about a turtle who wins a race against a rabbit.<end_of_turn>\\n<start_of_turn>model\\nLong ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\\n',\n",
              " 'input_ids': [2,\n",
              "  105,\n",
              "  2364,\n",
              "  107,\n",
              "  6974,\n",
              "  496,\n",
              "  3925,\n",
              "  1003,\n",
              "  496,\n",
              "  36100,\n",
              "  1015,\n",
              "  18779,\n",
              "  496,\n",
              "  7547,\n",
              "  2342,\n",
              "  496,\n",
              "  27973,\n",
              "  236761,\n",
              "  106,\n",
              "  107,\n",
              "  105,\n",
              "  4368,\n",
              "  107,\n",
              "  12059,\n",
              "  4578,\n",
              "  236764,\n",
              "  496,\n",
              "  4592,\n",
              "  27973,\n",
              "  39494,\n",
              "  657,\n",
              "  496,\n",
              "  5111,\n",
              "  36100,\n",
              "  236761,\n",
              "  2195,\n",
              "  75956,\n",
              "  580,\n",
              "  496,\n",
              "  17635,\n",
              "  5597,\n",
              "  236761,\n",
              "  669,\n",
              "  27973,\n",
              "  11536,\n",
              "  2793,\n",
              "  7531,\n",
              "  532,\n",
              "  12280,\n",
              "  531,\n",
              "  13420,\n",
              "  236761,\n",
              "  669,\n",
              "  36100,\n",
              "  7953,\n",
              "  9378,\n",
              "  236764,\n",
              "  2918,\n",
              "  684,\n",
              "  2918,\n",
              "  236761,\n",
              "  2640,\n",
              "  506,\n",
              "  9009,\n",
              "  1757,\n",
              "  236764,\n",
              "  506,\n",
              "  36100,\n",
              "  24508,\n",
              "  1171,\n",
              "  236761,\n",
              "  669,\n",
              "  27973,\n",
              "  50962,\n",
              "  2311,\n",
              "  5226,\n",
              "  236761,\n",
              "  669,\n",
              "  36100,\n",
              "  35240,\n",
              "  236764,\n",
              "  11307,\n",
              "  529,\n",
              "  31245,\n",
              "  532,\n",
              "  16932,\n",
              "  6555,\n",
              "  236761,\n",
              "  106,\n",
              "  107],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(trainer.train_dataset[10][\"input_ids\"]))"
      ],
      "metadata": {
        "id": "_Ug9_ybXIkRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ac4815-769a-4f2b-ab0a-76ec7f4432d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Write a story about a turtle who wins a race against a rabbit.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "bRw7ANnaIhfQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3302d6374d904575879dd8955d527827",
            "a69ce9c2636940c7a8059c3b34fb0044",
            "83c1f337bcd1483893b9662e74a25b3e",
            "fdfba40825214d7cbc7e061720ccee0e",
            "2f26672b40e24517b856c5bc081425ca",
            "ce9fcd11bcb84293a8bac57d06aca063",
            "a520348d84254da7b6b59636f5fd4c37",
            "f548decd9d41483db6bcd2f25f030c9f",
            "09d84c6d065240df973e82d75b1a0b1d",
            "012230814ca84f0588b900d558f73efd",
            "8a346c6f5ca64404a3121aefd9be18d0"
          ]
        },
        "outputId": "5d70783e-7c59-43b0-bace-49c537a8b131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3302d6374d904575879dd8955d527827"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[10][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ],
      "metadata": {
        "id": "_3CWqbNjIl63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cd92683f-e37b-403c-868f-c679fc44035b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                       Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "-1WPvsrhIvnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcc570dd-66f9-4685-b2be-3c8941471b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 10 | Total steps = 130\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 30,375,936 of 298,474,112 (10.18% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [130/130 00:35, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.278900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.942700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.388200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.987100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.628700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.835200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.366700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.085700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.198400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.982900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.957400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.942900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.833700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.994900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.988700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.827300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.711100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.790100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.944100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.591200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.789300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.441100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.791500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.302900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.462200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.388400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.397800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.613200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.451000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.249900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.226700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.446700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.268200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.989800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.250500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.317000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.974700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.182400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.154100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.112200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.952600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.033700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.860200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.079900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.094200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.860300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.965600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.630800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.593900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.806500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.788300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.680800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.554000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.564200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.532800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.664200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.709600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.595700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.543600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.614500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.477000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.724300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.485700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.506100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.333500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.541800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.533500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.450100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.406400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.439100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.466300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.363100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.347400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.551200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "tjASH4yKd9d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(user_prompt):\n",
        "  messages = [\n",
        "      {\"role\" : 'user', 'content' : user_prompt}\n",
        "  ]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = False,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "  ).removeprefix('<bos>')\n",
        "\n",
        "  from transformers import TextStreamer\n",
        "  return model.generate(\n",
        "      **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = 125,\n",
        "      temperature = 1, top_p = 0.95, top_k = 64,\n",
        "      streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "  )"
      ],
      "metadata": {
        "id": "FynVpsMPI9yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = gen(dataset['train']['conversations'][10][0]['content'])"
      ],
      "metadata": {
        "id": "3AoWGLqiPVW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526a24ac-adf2-455c-c372-d04b52d15ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A slow turtle hopped slowly and waited for the race. Suddenly, the rabbit ran in front of it, leaping high! The turtle froze, afraid it would fall. The rabbit cleared its throat and said, â€˜No.â€™ The turtle cheered, proud of patience and slow steps.<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(token_ids[0]))"
      ],
      "metadata": {
        "id": "FY6e12c2PV2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22da651-bf8f-4cfd-fe16-cb3445d9478c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Write a story about a turtle who wins a race against a rabbit.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "A slow turtle hopped slowly and waited for the race. Suddenly, the rabbit ran in front of it, leaping high! The turtle froze, afraid it would fall. The rabbit cleared its throat and said, â€˜No.â€™ The turtle cheered, proud of patience and slow steps.<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Artefacts"
      ],
      "metadata": {
        "id": "FCoug4rz8Iuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{LM_MODEL}-microfables\")"
      ],
      "metadata": {
        "id": "N46_M3Qa_scg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(f\"{LM_MODEL}-microfables\", tokenizer=tokenizer, save_method=\"lora\")"
      ],
      "metadata": {
        "id": "0khD2El8LzZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40123e6-587d-4e04-d4f1-903b46d81645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `gemma-3-270m-it-microfables`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `gemma-3-270m-it-microfables`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 8924.05it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/gemma-3-270m-it-microfables`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model.save_pretrained_gguf(f\"{LM_MODEL}-microfables-gguf\", tokenizer, quantization_method = \"q8_0\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError during GGUF conversion: {e}\")\n",
        "    print(\"Proceeding with manual conversion using llama.cpp\")"
      ],
      "metadata": {
        "id": "kknJ_Ata8GIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14d46fc-2eea-485c-8d21-79ab281ec1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Updating system package directories\n",
            "Unsloth: Install GGUF and other packages\n",
            "RuntimeError during GGUF conversion: Unsloth: `gemma-3-270m-it-microfables-gguf` does not exist?\n",
            "Proceeding with manual conversion using llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(HF_MODEL_REPO)"
      ],
      "metadata": {
        "id": "Fc-Y64x79qx8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "4e77dc36b470476c97e995b485faed88",
            "f01b687a6cf64c4b91e77d61414ff4c1",
            "4521bfc5cbeb47738426b9ba87911d51",
            "f0ac64d507394e2a938db28db31d162c",
            "a83a1bfe515e466ea399ed572c96f837",
            "e415b9a4045346f08761014792f5b866",
            "565bf600f1d242a6ba612690643bcd36",
            "1efc29acc329408fae46c4f40c1c599f",
            "5f382dbf4e5c460a9c82a54371296086",
            "bc59025321784124922cd840b24da208",
            "5781060771e74577abb244bd08cbe787",
            "837510addc844c0c82259483e9797268",
            "0d3e57573f044a8d8ff1d2c29c45825b",
            "3a9566c10fa1454bbb4a36967e8b2eb8",
            "e29c2b0ac349445eabf440618be5d16b",
            "76d7fb0967474f5da5c9187ce4c497f7",
            "c5609fe7b3944eea865ead9b7e1c767b",
            "7fac43ef517b4717b7bb32da068ae7ca",
            "bf162c539da64436a611c3c67009e69e",
            "c21f9cf9ef0742e0b8caa199102d1113",
            "3d105234ec524da79b8b8106ae211db1",
            "9a0d88c552a5486cb6a9f98168f85681",
            "f6074d12efb24e4ea8bbb9e4efc84f76",
            "7e073f6e293f4ec397d9a4e418f38072",
            "d438cc6bc37b4494939da666dd982b9a",
            "cbf3c434532a4ddcbd21341f6f9083ac",
            "885bf3686e274c1391491a4dfba3411a",
            "4bb10ae9e045467c9c3f5fed548b9454",
            "7ab95ce109274b12a14e158b8d5e7b79",
            "1708cef6489740bc80e2763cc2c5a9bb",
            "25fd51d988064f398d890e02e2d50f14",
            "2384db068ac54484b2b3d7d531ad1568",
            "e354bdd6d870426486f33d54f7a2cb59",
            "5178afbb19264dbfade39c37af25a15b",
            "02eb6145c8994aca8e47815817497dcd",
            "cdeeb0f4d3b74b9aa95f853620b57ed8",
            "5079c1bd606345de823aef1dc7f56bbf",
            "36cf402423d04bba9ec966453b567746",
            "48089e7f72c2419880131a1acf4f4f35",
            "b2c984e1ff5b4f87b6efd9b275388363",
            "4a0673e0dd644f5a868a6143456efb18",
            "086252050cc5411fb9fb8f666def92ce",
            "571c3a68d7b34096af3d52c2a3ebe14a",
            "27c63dcb125846ae9182dcc257d82e7c"
          ]
        },
        "outputId": "2ab271e7-66de-417f-b4ce-c71d9b33ec14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e77dc36b470476c97e995b485faed88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "837510addc844c0c82259483e9797268"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...mp4y5qejvi/tokenizer.json: 100%|##########| 33.4MB / 33.4MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6074d12efb24e4ea8bbb9e4efc84f76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...p4y5qejvi/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5178afbb19264dbfade39c37af25a15b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(HF_MODEL_REPO)"
      ],
      "metadata": {
        "id": "quZX0vEELp_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "3a57ec9164c44ddd84338080b2679280",
            "58f0d867f120425d9e1bba9c8ac1f560",
            "27f109105d2b42fea201ffca9d104c0e",
            "426f7aa847674bfeb3bfbeff143d5688",
            "e84202bcecc442658a6bcf479b17aa40",
            "b8c4287199834d68af8e66bd93115ef4",
            "cab5b894c8e64861ba38824397fab48d",
            "477dfc39f4344470b6a2a72bd7d21e49",
            "0b642dd3de1f49c187c3b1d93a954f6b",
            "7053a8e2db4744c392127531b138116b",
            "85ef010651f84d49ac21789cc30855bb",
            "1ae1c870bf424ad9b1b7e9a572ae5d12",
            "4ec3ab7dbc2d4a1ca3b69370bc1a9ba0",
            "802740d766f144b8818a548b4013fb71",
            "2d700c6258294c8c9e41dfdedb1d0584",
            "7accf28faaa54625b383bd6cc7e8f810",
            "6189396c6b0f411585a53067810502de",
            "354dd3698dea417399a43319c16078cd",
            "26836cb09d784cd7a52d9c61b3f25300",
            "1136ed51fb724989919211beffd2beca",
            "26381be0e2c94794b0a502b5124ecee1",
            "4747e6b8202344f29b3c225881b360ab",
            "d04a98bd723543198826c17b05372480",
            "a457ffb6c26245f8abc4422c6ffddb45",
            "07aeeab0865c4bc9a9d41b441a423936",
            "1ae9cc80afef40d3aeea055144543911",
            "0148bd67d7d2432389d2a10d6ef99c31",
            "b5a7a4ddbe564d62a97ae6d4193040ba",
            "20c1873bff794957aa975dd8f8977a71",
            "20fb75c69e3b49f590370e2f87c9daa9",
            "0093ee1e2a3b4619a11ef595e0529634",
            "331510ef551b455bad66bfae6315d6aa",
            "268f59bd8d07425399aa8725548e9c4f",
            "0dcce15bf1fe43d095db0286017aa39d",
            "b77c4df57d4f438b8773849fde682c92",
            "0f4db6a151ba4404a12100e1ab6c0339",
            "ec0859eb34114db4abb516857991334d",
            "09cf8316a7814dc9a876234d5243b701",
            "819502dad11a47488c1f185364ac7121",
            "784a257416a543ea8ca24bebf76776f1",
            "73d61f6a9b644b1f84a2d4537bca142d",
            "c1e074aebbb64118b58a81f130e319ef",
            "05c20ff44e8b45efa60489cfa36a3b6f",
            "e9db8acf8e684cb18af66aa9eb722e0a"
          ]
        },
        "outputId": "db01a1dd-e7cc-4ed7-bea1-2ca69a9da6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a57ec9164c44ddd84338080b2679280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ae1c870bf424ad9b1b7e9a572ae5d12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d04a98bd723543198826c17b05372480"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...adapter_model.safetensors:   0%|          |  559kB /  122MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dcce15bf1fe43d095db0286017aa39d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to https://huggingface.co/mayurmadnani/gemma-3-270m-microfables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "nudCsB4mleAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.save_pretrained(f\"{LM_MODEL}-microfables-merged\")\n",
        "tokenizer.save_pretrained(f\"{LM_MODEL}-microfables-merged\")"
      ],
      "metadata": {
        "id": "RAs_1HxBn1Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f42ace0-f124-445a-905f-e994697ebef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gemma-3-270m-it-microfables-merged/tokenizer_config.json',\n",
              " 'gemma-3-270m-it-microfables-merged/special_tokens_map.json',\n",
              " 'gemma-3-270m-it-microfables-merged/chat_template.jinja',\n",
              " 'gemma-3-270m-it-microfables-merged/tokenizer.model',\n",
              " 'gemma-3-270m-it-microfables-merged/added_tokens.json',\n",
              " 'gemma-3-270m-it-microfables-merged/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral-common"
      ],
      "metadata": {
        "id": "IiuZoUHZhMiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e63a1337-9fc7-406e-fad9-833e825626a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral-common\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.11.9)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.2)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral-common\n",
            "Successfully installed mistral-common-1.8.5 pycountry-24.6.1 pydantic-extra-types-2.10.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert_hf_to_gguf.py gemma-3-270m-it-microfables-merged --outfile gemma-3-270m-it-microfables-merged.gguf --outtype f16"
      ],
      "metadata": {
        "id": "SzAHTbUDogjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0472f73-431e-401f-ef31-2c745c18884b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: gemma-3-270m-it-microfables-merged\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float16 --> F16, shape = {640, 262144}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> F16, shape = {2048, 640}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> F16, shape = {640, 2048}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> F16, shape = {1024, 640}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> F16, shape = {640, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> F16, shape = {640, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float32 --> F32, shape = {640}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 106\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{ '<start_of_turn>model\n",
            "' }}\n",
            "{%- endif -%}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:gemma-3-270m-it-microfables-merged.gguf: n_tensors = 236, total_size = 536.3M\n",
            "Writing: 100% 536M/536M [00:01<00:00, 333Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to gemma-3-270m-it-microfables-merged.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can also use [gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) to create GGUF quants"
      ],
      "metadata": {
        "id": "0NXJBFjrIt1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serving"
      ],
      "metadata": {
        "id": "wAdjsVh0RNUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "zgWjeXoXRLvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96781d7f-095c-4baf-e648-8b74d327c72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  39554      0 --:--:-- --:--:-- --:--:-- 39644\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama serve in the background\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'])\n",
        "\n",
        "# Give the processes a moment to start\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "B2QE7EePzDlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelfile_content = tokenizer._ollama_modelfile\n",
        "\n",
        "modelfile_content = modelfile_content.replace(\"FROM {__FILE_LOCATION__}\", \"FROM gemma-3-270m-it-microfables-merged.gguf\")\n",
        "\n",
        "with open(\"Modelfile\", \"w\") as f:\n",
        "    f.write(modelfile_content)"
      ],
      "metadata": {
        "id": "9fK-CHRDHWs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama create gemma-3-270m-microfables -f ./Modelfile"
      ],
      "metadata": {
        "id": "UJzj8bDaUTw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad69c83-2a10-4c25-91cc-bba1607fa5cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbA4blvEG2Ch",
        "outputId": "f81780af-23fa-4b94-c424-c9bb14bb07f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                               ID              SIZE      MODIFIED               \n",
            "gemma-3-270m-microfables:latest    4b33351e911a    542 MB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434/api/generate -d '{ \\\n",
        "  \"model\": \"gemma-3-270m-microfables\", \\\n",
        "  \"prompt\": \"Write a short story about a brave knight.\", \\\n",
        "  \"stream\": false \\\n",
        "}'"
      ],
      "metadata": {
        "id": "mvmR9GBrVENy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2d4e86-1568-4b2c-e209-6386c4ddf7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma-3-270m-microfables\",\"created_at\":\"2025-10-09T18:38:54.223326207Z\",\"response\":\"In a dark castle, a knight stood brave. When a dragon rose, the knight rode fast. He faced the dragon and lifted his sword. The dragon roared in pain, and the knight held his ground. Together, they won the battle and saved the kingdom.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[105,2364,107,6974,496,2822,3925,1003,496,36711,52482,236761,106,107,105,4368,107,902,496,4996,23986,236764,496,52482,15032,36711,236761,3026,496,25800,11076,236764,506,52482,38965,4592,236761,1293,17175,506,25800,532,32340,914,26114,236761,669,25800,149193,528,4331,236764,532,506,52482,4247,914,3866,236761,29743,236764,901,2810,506,10041,532,10683,506,21880,236761],\"total_duration\":3818751961,\"load_duration\":2562812954,\"prompt_eval_count\":18,\"prompt_eval_duration\":662161714,\"eval_count\":54,\"eval_duration\":592648345}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama cp gemma-3-270m-microfables mayurmadnani/gemma-3-270m-microfables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN8iNTk5SFtR",
        "outputId": "58baa04c-31d8-432e-a314-9a0f267a9679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copied 'gemma-3-270m-microfables' to 'mayurmadnani/gemma-3-270m-microfables'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama push mayurmadnani/gemma-3-270m-microfables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgqcs9ooBvty",
        "outputId": "3aeade97-e826-4b72-c92c-98b2afedada1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            "You can find your model at:\n",
            "\n",
            "\thttps://ollama.com/mayurmadnani/gemma-3-270m-microfables\n"
          ]
        }
      ]
    }
  ]
}