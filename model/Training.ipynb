{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "hfskCiZOupgy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LM_MODEL = \"gemma-3-270m-it\"\n",
        "HF_MODEL_REPO = \"mayurmadnani/gemma-3-270m-microfables\""
      ],
      "metadata": {
        "id": "Smvan32oXw0I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "8nQnRb8bdTUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "zCSHjP2iG92d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = f\"unsloth/{LM_MODEL}\",\n",
        "    max_seq_length = max_seq_length, # for long context\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False,\n",
        ")"
      ],
      "metadata": {
        "id": "1K6hgVXfHGCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464,
          "referenced_widgets": [
            "1c9db94d19164fa0ba25fb4f694d5d07",
            "cf376e3a574b4c85989a21cdc2db3534",
            "0873a30091354a0f9ded02aa387f1c30",
            "032d11e15cfd4891bb90fb706eca7cbd",
            "1096f6908b86461aac9530acb64b8670",
            "44919a3b568b4f838de4264adb46ae11",
            "53a5ade77842427ba07336cde6bf44a5",
            "392d48f5fcf8418d8334c30e81e7a4ee",
            "69712daba33f4f289df90acc5ed85db0",
            "2e927d28cad44b3b95cc7af68577fead",
            "33b75d64025a48c69c026e75047cdae8",
            "08b50860955048f596272d171a827487",
            "0eea3ac0e46949a790b70b0fddd0d94f",
            "3c4452669cd74ae1840b563e265ddddf",
            "4adeef8a29644044994ab39a13435548",
            "04ce32a80e204f8599838c3b5f5ba40d",
            "9d4e2f550b7b4bfb89c75bbb968f06df",
            "4b278f928cb4417a91e05592cbff8e6b",
            "a4e5c844cdcd4372b726e8811c724d55",
            "ad5285e4cb8d44b38f5350b0066a6d9b",
            "6c1a15fc4f644b7793df8d4eba3a8383",
            "26f36004985946dc940c91edb459b991",
            "2e9aa5c439ea475ebe6fc4858a5cfe75",
            "c73ca3ad8d324195bd08e020efdc71a1",
            "5ee79081b06e48c19db7f11fab4d4d55",
            "28b77a5d88fd4bd3a5d99b6a9b661bc5",
            "25ab99e9dd0c4f4694aa87df6c9d1ec0",
            "773ae2f43f0e4c5991c73dc3f62f1cd1",
            "595ca9453d674573851f441c61cf6c11",
            "9c02836e37bd4a509b21afa644528b78",
            "28a270837552458cb691cd8b2eab6558",
            "141840c555f940cfa71905335e01cf95",
            "b7bc483bea7d48629525c14a9a8a3854",
            "0d1bf9852ea84d6882e184f3d34d7d40",
            "3990622111694555b2d11df6305f92af",
            "a99c528db34b4f31a6f571ffe166d997",
            "61777131423f48d19b989f43549832e1",
            "c5a3ebd9da774afe9b4a67114e80ccd0",
            "0f596ccf0e054d6abb9a414d8c8d1c69",
            "dde229bbf3d340aaa45b854edaa33706",
            "6e41754b60534e1a89495731d33798d1",
            "bc422030e5f841f8b51620d2eae161f4",
            "6ef24e6d98624133984e2790f44fcfbe",
            "a7c8ccb53e5d4f9883a3933bac8cec37",
            "2cad319778094a378926503c65390636",
            "47371febbbeb470aa1d35fa80d14ea96",
            "db0b685fb39b4934a41ae659984f5e9f",
            "b2a6b11aae614130b56e767967707e55",
            "191f080b79894047bb4158b28bd4be64",
            "fe661d6d6bd344669ae2610abd8a875c",
            "e332708533734ee4a8b66ecda01837f6",
            "49679385cc9e406b8289941a161d29da",
            "25930e23c319499688fd1b67544cc07c",
            "72caf5ab066340a4aae4566d5020e172",
            "e753d5587ff24899800ba837ecbd86d4",
            "724d659f575643309913634535cda53c",
            "07a34bfd5c1e4efb95d0e72364630460",
            "04e93aadf9d7478fb2404a2a783f04ff",
            "f7fd4175625b41f88710d809e57da51b",
            "ab52cd715ebe4f2ab0f4975dac14626e",
            "7480a652aa6544c6acbdd5f8b000089b",
            "71c7502c9c6e432aaaf20bf7aff5670f",
            "965bb0c01f35413d9da2eebf2e1c634c",
            "6151b03231fe4b05a737bb2d42421bbe",
            "4af6ae5a6cbc4a2996e137ab785a3fcb",
            "5b0c7abbd00a4aab934fa953dce2ae21",
            "f1f7cdbd2ea24f0c9a332a7c71e5a6f4",
            "5ccfaf59c7394f358b8168583818f145",
            "2c2841026c234895a437cdd383f78d6f",
            "cf308f8828e54feba20368e9434859f3",
            "09fe52ca91a540c7b15c89cf72cdd9f4",
            "d50cc8bac8f84bb3a632866299aaac3e",
            "c8274ed924b2492faf183d0a48111677",
            "df5acb693a2641129071b3d68826e552",
            "52b73617f1d24d8cbddcaecb18fa7568",
            "920529560e644f32b62d9bf8269defeb",
            "9aa375eab27c42b38e15af8025ba3bb2",
            "4defee48383e49009ce35f1eddce1772",
            "6907f7b34e634596a130d2629f62b5fa",
            "177cf453f2a24a08a232f53b8eae7749",
            "3867b9f8c24640c594040ae1fe49e0cd",
            "b61569e6d4fc4fd395df47d704f87d45",
            "27acdb2c404847fd953e64b041a5e64c",
            "686764f72ab440b9bb957757d5c10045",
            "06ed0b06fafb46139c15cbba407c4b33",
            "e7a76ed33e02443ea411aa542c1f4bc6",
            "9b0a9ff5231042f78f8af9a7d3c8e0ab",
            "e59b24073b8d42bb8c9b357ae0aec8c1"
          ]
        },
        "outputId": "dfd27ccd-6709-446d-d541-4c24766dd56e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.10.12: Fast Gemma3 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c9db94d19164fa0ba25fb4f694d5d07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08b50860955048f596272d171a827487"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e9aa5c439ea475ebe6fc4858a5cfe75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d1bf9852ea84d6882e184f3d34d7d40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cad319778094a378926503c65390636"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "724d659f575643309913634535cda53c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1f7cdbd2ea24f0c9a332a7c71e5a6f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4defee48383e49009ce35f1eddce1772"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "uc2j-Dp7HbYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fac678b-568e-497c-cdbd-09e9d577beab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma3ForCausalLM(\n",
              "  (model): Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-17): 18 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{LM_MODEL}-base\")"
      ],
      "metadata": {
        "id": "Uv5a4vD7_20g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # rank stabilized LoRA\n",
        "    loftq_config = None, # LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "IUz0HdSsIN7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b5f86c-a9df-4576-fce3-62ea6b023bb0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "7J97k2vWHWFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772c3c85-fb05-4dcc-cba8-51236f32b5e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma3ForCausalLM(\n",
              "      (model): Gemma3TextModel(\n",
              "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x Gemma3DecoderLayer(\n",
              "            (self_attn): Gemma3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1024, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=640, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "            )\n",
              "            (mlp): Gemma3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=640, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=640, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=128, out_features=640, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (rotary_emb): Gemma3RotaryEmbedding()\n",
              "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "p6qi4wjb6O1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma3\",\n",
        ")"
      ],
      "metadata": {
        "id": "zFXBy9IiIYEp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.get_chat_template())"
      ],
      "metadata": {
        "id": "HloPxqFXFKgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012440a7-7bd5-4498-ae03-bc12fd9aba17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{ '<start_of_turn>model\n",
            "' }}\n",
            "{%- endif -%}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files= \"stories_dataset.jsonl\")"
      ],
      "metadata": {
        "id": "mJ9tAoBkIYfB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "501347d3627346578c02d4fdad104f89",
            "f67f367ee5e74a3f93030d896cb6d26a",
            "910729fea58145c796774876ba4e6619",
            "852dc9334f624051add663e732ce0222",
            "2cc78e4d844648ff8b7c3aaa85beb960",
            "d68e11a4a67746f5ae8dafcc9ec3b226",
            "f0aa9d84c83b4e39a4c78d47a84dc1af",
            "50c2e897a36146eca78bf5f1c153dd8e",
            "079db660d8cc44ad9c229de64d150054",
            "5ae3241fc3024db8a6000d40d6555d59",
            "3ac8fcce0f3045efb3654412a48968fe"
          ]
        },
        "outputId": "490c9905-2138-4738-e186-d5842ce814d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "501347d3627346578c02d4fdad104f89"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_chatml(example):\n",
        "    return {\n",
        "        \"conversations\": [\n",
        "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(\n",
        "    convert_to_chatml\n",
        ")"
      ],
      "metadata": {
        "id": "7wKj2TiJIZii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0315894e268b4ef9a472a9d5f5e4a13b",
            "a1cb70c3e635412e8468f81ebd928678",
            "9ef0a12b1543452e92933229d9eea550",
            "9c03f510b3e84f68ad28ea29e3efdb32",
            "905cbd6416e640769434dc4e009d17c6",
            "7a6ab77693fd4ee3b483aacbba6e8d85",
            "a206bc650dc447c98175b4c65627e739",
            "299e04ba4e134424bae9628a4651b06b",
            "398feaf0e8c645e78a9785ea5ec7e602",
            "3e3a0885234741989093f13ca11b1b58",
            "0501f89da7634afeaf909dd8a458a1c1"
          ]
        },
        "outputId": "f5a95401-887e-40ca-d80e-8bc5864c3bd6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0315894e268b4ef9a472a9d5f5e4a13b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][10]"
      ],
      "metadata": {
        "id": "TRR9vXgQIbX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a540d17-821d-43a2-e398-7417ebbd41e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              " 'response': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              " 'conversations': [{'content': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ],
      "metadata": {
        "id": "EdJZCZUNIc2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ea5ed743d2ba49128e488f0a64d21311",
            "97ff0003479e4c9889de5aa7e34331d8",
            "f86314b87e0b456c8f36f0398f0f7957",
            "f000a44e69404fc0b713f04a1672b3ad",
            "f6d68334fe804adebd12ca1c81c80da3",
            "b591053ea91d4c0da07a55a32cbf8d6f",
            "6466ac27634349639baadc387c3184c5",
            "b42d89c98f274afe97aa4803971e69e5",
            "a2fed55b6b35424690e7df5cb9d909e8",
            "f1f405e8b2024cf18b9832e717be46ec",
            "273fb2e409e74b31b1cb76af02cf2955"
          ]
        },
        "outputId": "5ac21718-2d34-4f39-eb41-e70d2df63700"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea5ed743d2ba49128e488f0a64d21311"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][10]['text'])"
      ],
      "metadata": {
        "id": "qHRdqu53IerA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86c8982-7f5b-45df-967a-a53cde16a8fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "Write a story about a turtle who wins a race against a rabbit.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(f\"{LM_MODEL}-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dDT2Tr9AAP9",
        "outputId": "84c63b18-5227-44f8-ae18-c0aa3109ed73"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gemma-3-270m-it-base/tokenizer_config.json',\n",
              " 'gemma-3-270m-it-base/special_tokens_map.json',\n",
              " 'gemma-3-270m-it-base/chat_template.jinja',\n",
              " 'gemma-3-270m-it-base/tokenizer.model',\n",
              " 'gemma-3-270m-it-base/added_tokens.json',\n",
              " 'gemma-3-270m-it-base/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune"
      ],
      "metadata": {
        "id": "4odXsSP3dj3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 10,\n",
        "        # max_steps = 50,\n",
        "        learning_rate = 5e-5,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "zdl4gvrBIfuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ccae5b6eec4b42098698f3c4326f9feb",
            "31ef9147c0c445a7aa01245626bf0951",
            "b6d0a348cba24870b51badecf6038424",
            "319c3fe3fc14422d831d852cff11ae9f",
            "6edc19f079864185969caf43233e5ad8",
            "73c6c888b5e142338b8729ba36d8cd9a",
            "2b44545c32cf458cb7c2e5d8d7d2cc65",
            "5bd3f63cd3da48158bf4fac7fd7e56c0",
            "3ebe299f7bf84e7dbe77535f58b5e118",
            "bb3f2a7885d947579c7e1272da15fb6a",
            "8f10354c29464b42842b6f2d89ce0b84"
          ]
        },
        "outputId": "48c22f55-e842-4693-a8a3-814c6b276348"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccae5b6eec4b42098698f3c4326f9feb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma4q6cozHGEF",
        "outputId": "39158d41-e626-4b11-b1b2-efa952971391"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              " 'response': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              " 'conversations': [{'content': 'Write a story about a turtle who wins a race against a rabbit.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.',\n",
              "   'role': 'assistant'}],\n",
              " 'text': '<start_of_turn>user\\nWrite a story about a turtle who wins a race against a rabbit.<end_of_turn>\\n<start_of_turn>model\\nLong ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\\n',\n",
              " 'input_ids': [2,\n",
              "  105,\n",
              "  2364,\n",
              "  107,\n",
              "  6974,\n",
              "  496,\n",
              "  3925,\n",
              "  1003,\n",
              "  496,\n",
              "  36100,\n",
              "  1015,\n",
              "  18779,\n",
              "  496,\n",
              "  7547,\n",
              "  2342,\n",
              "  496,\n",
              "  27973,\n",
              "  236761,\n",
              "  106,\n",
              "  107,\n",
              "  105,\n",
              "  4368,\n",
              "  107,\n",
              "  12059,\n",
              "  4578,\n",
              "  236764,\n",
              "  496,\n",
              "  4592,\n",
              "  27973,\n",
              "  39494,\n",
              "  657,\n",
              "  496,\n",
              "  5111,\n",
              "  36100,\n",
              "  236761,\n",
              "  2195,\n",
              "  75956,\n",
              "  580,\n",
              "  496,\n",
              "  17635,\n",
              "  5597,\n",
              "  236761,\n",
              "  669,\n",
              "  27973,\n",
              "  11536,\n",
              "  2793,\n",
              "  7531,\n",
              "  532,\n",
              "  12280,\n",
              "  531,\n",
              "  13420,\n",
              "  236761,\n",
              "  669,\n",
              "  36100,\n",
              "  7953,\n",
              "  9378,\n",
              "  236764,\n",
              "  2918,\n",
              "  684,\n",
              "  2918,\n",
              "  236761,\n",
              "  2640,\n",
              "  506,\n",
              "  9009,\n",
              "  1757,\n",
              "  236764,\n",
              "  506,\n",
              "  36100,\n",
              "  24508,\n",
              "  1171,\n",
              "  236761,\n",
              "  669,\n",
              "  27973,\n",
              "  50962,\n",
              "  2311,\n",
              "  5226,\n",
              "  236761,\n",
              "  669,\n",
              "  36100,\n",
              "  35240,\n",
              "  236764,\n",
              "  11307,\n",
              "  529,\n",
              "  31245,\n",
              "  532,\n",
              "  16932,\n",
              "  6555,\n",
              "  236761,\n",
              "  106,\n",
              "  107],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(trainer.train_dataset[10][\"input_ids\"]))"
      ],
      "metadata": {
        "id": "_Ug9_ybXIkRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d536dfa-5525-4bdb-e3a3-564e7f5b3a27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Write a story about a turtle who wins a race against a rabbit.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "bRw7ANnaIhfQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6acb17e0f3b449ffa058cb12e23fdd2e",
            "e29549f8e21040c591b771a4d0fd5570",
            "f3f3f510292345a2a5379dbe54f5859f",
            "2feb2a7f9b0a43ccb53f51379d1281aa",
            "2390d01154fc4812b70bd3d85a142f12",
            "1b5d46f49d59440c8e518bac82aa4e5a",
            "81fc405234c14b5da0107c61d3c19984",
            "55309112d30c47caaa25b1286b9b1f45",
            "d2a1cdc229144109a9d8bf7277c68d5e",
            "c89bf162f6b74e3e8fd122c6f811ecfc",
            "025b1c22e0594ee1888c47fee59c2391"
          ]
        },
        "outputId": "404335a9-11d5-4020-a1c3-30e2da1519db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=6):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6acb17e0f3b449ffa058cb12e23fdd2e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[10][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ],
      "metadata": {
        "id": "_3CWqbNjIl63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dd8201a4-4263-4bae-be7c-749b2e8f5c4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                       Long ago, a fast rabbit laughed at a slow turtle. They raced on a sunny morning. The rabbit ran far ahead and stopped to nap. The turtle kept walking, step by step. At the finish line, the turtle crossed first. The rabbit woke too late. The turtle smiled, proud of patience and steady steps.<end_of_turn>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "-1WPvsrhIvnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfaff5ad-f266-4663-b315-eabaf4a65d2d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 10 | Total steps = 130\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 30,375,936 of 298,474,112 (10.18% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [130/130 00:36, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.278900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.942700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.388200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.987100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.628700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.835200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.366700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.085700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.198400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.982900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.957400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.942900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.833700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.994900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.988700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.827300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.711100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.790100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.944100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.591200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.789300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.441100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.791500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.302900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.462200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.388400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.397800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.613200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.451000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.249900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.226700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.446700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.268200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.989800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.250500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.317000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.974700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.182400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.154100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.112200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.952600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.033700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.860200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.079900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.094200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.860300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.965600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.630800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.593900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.806500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.788300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.680800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.554000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.564200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.532800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.664200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.709600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.595700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.543600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.614500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.477000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.724300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.485700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.506100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.333500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.541800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.533500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.450100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.406400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.439100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.466300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.363100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.347400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.551200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "tjASH4yKd9d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(user_prompt):\n",
        "  messages = [\n",
        "      {\"role\" : 'user', 'content' : user_prompt}\n",
        "  ]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = False,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "  ).removeprefix('<bos>')\n",
        "\n",
        "  from transformers import TextStreamer\n",
        "  return model.generate(\n",
        "      **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = 125,\n",
        "      temperature = 1, top_p = 0.95, top_k = 64,\n",
        "      streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "  )"
      ],
      "metadata": {
        "id": "FynVpsMPI9yL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = gen(dataset['train']['conversations'][10][0]['content'])"
      ],
      "metadata": {
        "id": "3AoWGLqiPVW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de2944b-382c-4e51-94e7-a58d1647104c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A slow turtle hopped slowly and waited for the race. Suddenly, the rabbit ran in front of it, leaping high! The turtle froze, afraid it would fall. The rabbit cleared its throat and said, â€˜No.â€™ The turtle cheered, proud of patience and slow steps.<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(token_ids[0]))"
      ],
      "metadata": {
        "id": "FY6e12c2PV2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e2a442-f422-45c0-b540-9a938d5d4007"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Write a story about a turtle who wins a race against a rabbit.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "A slow turtle hopped slowly and waited for the race. Suddenly, the rabbit ran in front of it, leaping high! The turtle froze, afraid it would fall. The rabbit cleared its throat and said, â€˜No.â€™ The turtle cheered, proud of patience and slow steps.<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Artefacts"
      ],
      "metadata": {
        "id": "FCoug4rz8Iuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{LM_MODEL}-microfables\")"
      ],
      "metadata": {
        "id": "N46_M3Qa_scg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(f\"{LM_MODEL}-microfables\", tokenizer=tokenizer, save_method=\"lora\")"
      ],
      "metadata": {
        "id": "0khD2El8LzZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e955aeb8-7c55-4478-a107-a395fa1c2875"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `gemma-3-270m-it-microfables`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `gemma-3-270m-it-microfables`\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `gemma-3-270m-it-microfables`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 59.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `gemma-3-270m-it-microfables`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11814.94it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/gemma-3-270m-it-microfables`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(f\"{LM_MODEL}-microfables-gguf\", tokenizer, quantization_method = \"q8_0\")"
      ],
      "metadata": {
        "id": "kknJ_Ata8GIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b85ad78-0faf-4d18-d575-38769d33ec4a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging model weights to 16-bit format...\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `gemma-3-270m-it-microfables-gguf`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `gemma-3-270m-it-microfables-gguf`\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `gemma-3-270m-it-microfables-gguf`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 120.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `gemma-3-270m-it-microfables-gguf`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13189.64it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/gemma-3-270m-it-microfables-gguf`\n",
            "Unsloth: Converting to GGUF format...\n",
            "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF f16 to ['q8_0'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: Updating system package directories\n",
            "Unsloth: All required system packages already installed!\n",
            "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
            "Unsloth: Cloning llama.cpp repository\n",
            "Unsloth: Install GGUF and other packages\n",
            "Unsloth: Successfully installed llama.cpp!\n",
            "Unsloth: Preparing converter script...\n",
            "Unsloth: [1] Converting model into f16 GGUF format.\n",
            "This might take 3 minutes...\n",
            "Unsloth: Initial conversion completed! Files: ['gemma-3-270m-it.F16.gguf']\n",
            "Unsloth: [2] Converting GGUF f16 into q8_0. This might take 10 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Model files cleanup...\n",
            "Unsloth: All GGUF conversions completed successfully!\n",
            "Generated files: ['gemma-3-270m-it.Q8_0.gguf']\n",
            "Unsloth: example usage for text only LLMs: llama-cli --model gemma-3-270m-it.Q8_0.gguf -p \"why is the sky blue?\"\n",
            "Unsloth: Saved Ollama Modelfile to current directory\n",
            "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'save_directory': 'gemma-3-270m-it-microfables-gguf',\n",
              " 'gguf_files': ['gemma-3-270m-it.Q8_0.gguf'],\n",
              " 'modelfile_location': '/content/Modelfile',\n",
              " 'want_full_precision': False,\n",
              " 'is_vlm': False,\n",
              " 'fix_bos_token': True}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(HF_MODEL_REPO)"
      ],
      "metadata": {
        "id": "Fc-Y64x79qx8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "c59ce04140f44b20a63470dcdb79e9d0",
            "e4fafaf3646047ee951fc6b82f4fa5dd",
            "7d049eca864b47b7af1cdfe102bf372b",
            "64aac20eb19747639b716406501ffbc1",
            "a118e0c8260e4491a747981429eece7b",
            "1b648b14d28e414aaa2b137f7b4d388f",
            "ea393bd3ab634f51a76eb94ff798269a",
            "aad75456eb39435d8f51d3b1e855cc76",
            "258f47de436141a28ab83634a58c3e92",
            "ffcfe70ea85644eaa4a5ec738624f1f8",
            "fe2ac8c2c78e49ecab83b36f0bbc627f",
            "2974fa55aaf240128dcface8398fccd0",
            "fc77b0e2739e4527a7cdb97d62ec6ef6",
            "26598c61cf4d41658987af12a6fffb41",
            "66f96d6140164428a8a4877ce25cc866",
            "a6bbbdbafeb6487087e0ea8153392d71",
            "41c1365b20194120afed384156d9110b",
            "a1a815195f6445d19263dd12f0cc6edf",
            "070f34c04efb43fda1dd3ae6b6d33711",
            "1fe552619f4349f2982b60b264f23dda",
            "1cdb1406d7794b4f8a9210d577763f66",
            "a2552ac85fef4331808753a30f7fbabc",
            "62da1643aabf457f93fc8336f912cf3a",
            "13bda60a77c14702b6689251bf8fafff",
            "e4329ffdd79b4fbca2cac832eb6f76a2",
            "4ce60f1b94594125b6ee3e46cac155b3",
            "4303342bcdcf4375abccf4e216bbdcdf",
            "18bbff3b74674534908ab67cdac98b56",
            "5089f5e419b24adb872ea85350faf59f",
            "ecb9017b31fd46da9e99ee95814c1329",
            "fcfad8d7daed4b68abdb212e26379570",
            "080c90577edc4ec8b0c425a1d012ae6a",
            "bc063b90bfbc463dbabea7d70db76c7e",
            "4de238cfa9124d6cba087fa51ae36c1d",
            "01c55ad6a85b49169ad0d81d5ceb9578",
            "f5ca1a43c9884358a8cf9b2ef37386db",
            "b25d369144da44b58832b4ff4534aaba",
            "75ac79d72289479e8c1fcae01d93a3d9",
            "69bcf82f83b94f8ca5f8d3a3decbc174",
            "c5581a3398f64781b34ef6b5c1bfb33f",
            "6af011366d4c4b27a115eee6d0fbc9c6",
            "7d227892edc742519e5c920d1d7784b5",
            "17364bad65684257bb24fe64db5879f8",
            "171abaf7908d44c4a5b3a253d99dac90"
          ]
        },
        "outputId": "2885b055-4968-477f-f24c-2b3b441d987b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c59ce04140f44b20a63470dcdb79e9d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2974fa55aaf240128dcface8398fccd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...mp2q_z48hq/tokenizer.json:  75%|#######5  | 25.2MB / 33.4MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62da1643aabf457f93fc8336f912cf3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...p2q_z48hq/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4de238cfa9124d6cba087fa51ae36c1d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(HF_MODEL_REPO)"
      ],
      "metadata": {
        "id": "quZX0vEELp_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "1d6dd231d97f4912af7a6d75d5ab9797",
            "1141d41846dc4fb0a19c437f29a47329",
            "a2da43d66c194d8abb820016bab27ca1",
            "63911148a85a40cd889ba6953cf6d6e9",
            "ddb8d1c953cf43d8b3075098943c912d",
            "7d820c3403b2446da11e0bb6ccc7a7a1",
            "16711f6018ff4dec8868241f71bbb954",
            "83b36a49d34c4e2eba8cf05a65b2f25c",
            "fd9f73a9e9cb4518bbac2905e30317c2",
            "9eff036e32a946279b904eb3369d5da5",
            "36f67600c970415086c765ab2d25ee4a",
            "822c9824337f4ae2b46786a572f3c621",
            "f97fe0c8b5e84996b9746867d231845e",
            "de5814aaa8b04ad1aaed11c75f822375",
            "2905aa15f93d4e4b9eb3ed167371a774",
            "bc05abfe2b7f4acea3c85a225c02a09d",
            "200eaacd227f437db18226f188e072be",
            "7601ec92411a4d139df2712613fd701f",
            "e91aecd1a6f846e8b13eec1be85bc304",
            "0cec86638b6f41af8df4ad049303e737",
            "f698d6fe927e4bbdb95497b87ca1b6f8",
            "8b70a414fd8f4f17a639b3420f07b402",
            "8acb75f2ccb544e6a472c4e50d28acb5",
            "6bf1f502daba41a783ba99591b3691fc",
            "c173a0c18e3a4df8a29cbc7d60d771ef",
            "cf17ee0024f84028a54f13d76f16d020",
            "3248d3b7f2694c71aacc17eaa2320b8c",
            "77cf2f304ffb4a92bedfef0b779cf8e4",
            "04ee85d8020e48d386ee78579eea21af",
            "a1004e50410f4555bb19e05c21985989",
            "8be71154d9cd48e8839da9cb03f0c5a3",
            "a4a82ac6fe2948c0a5fadb7468101fe2",
            "c3c713bcf5ce4a5d908da05925b95e94",
            "822d36a858c64fea80ea7e6d82554741",
            "32d15cb5fe56408ea0322face7e4811d",
            "66eb99b71fd64fb18b16a9825d88e924",
            "8b1c58ecc6bd4056abfc66cea3b89221",
            "5030acfc1eb04a999637e267785d04aa",
            "17dfa3039d4747d398918aeb434e5a29",
            "7e39230566a2462b9470a2a31db5c4b4",
            "ed8bda327ec3481c9b148a01350a68e4",
            "a0ac9e65d864468397308e9c8390a88f",
            "c29aa04e779c414181229b5f1c551211",
            "fe9c9b40d85f4b3db5d17fd8a492c770"
          ]
        },
        "outputId": "5bd3fe77-8bc1-4efa-af1e-a69eeb373cdd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d6dd231d97f4912af7a6d75d5ab9797"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "822c9824337f4ae2b46786a572f3c621"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8acb75f2ccb544e6a472c4e50d28acb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...adapter_model.safetensors:  41%|####1     | 50.3MB /  122MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "822d36a858c64fea80ea7e6d82554741"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to https://huggingface.co/mayurmadnani/gemma-3-270m-microfables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(HF_MODEL_REPO, tokenizer, quantization_method = \"Q8_0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843,
          "referenced_widgets": [
            "d8b61e1f9105450faa474bac6cbbb69b",
            "5d76020043c9457ba5d30f61f3842c7f",
            "082b05d1b82344ed8d955cd9ec92c04a",
            "feaa150e09ce4b5dbd499bcb8b3f88a6",
            "13b57f40eeb1404d84a49a7c73eda4a0",
            "45dd969f94a042b2bb132411ba82b83a",
            "0c085e685ea64fe682366b65fb1f21ea",
            "1bbdeec1d8c44c428bbc58fd8fd528b6",
            "d6aba5277aa54f3d8065d91febb97067",
            "9d51179c2fef417d92ce193f4ccd8536",
            "0c4b6511190943ce87ae1011731d37aa",
            "908c70fd83414a659d9ee55900c0c828",
            "c97b326a51df449e824389c7080a97ea",
            "798e2dcc9f134658aa811d286127592b",
            "835148c0049749cebb10fd5d37831389",
            "1f6b02b1fbc54330b926c7162691dc5a",
            "41a435b020c44c7a8ceff318c90b3d72",
            "8a1b6ee5e87a4fcea495eb9420b536af",
            "8656fc8043a54541aad11d01b8ec2861",
            "07b7a0864d3c4a99a47e858734272f9f",
            "bf79d15bc4ed43a5a63f2c1548fdc181",
            "ff4782113f354c5f8a5f8471c78a9715",
            "6c25c3853e2e4e98a028d9e8af803021",
            "d2669fcdb8f24bd0b9f7576c2175b1a5",
            "b06f90bd148a450d991fec29a0fd77df",
            "ef62b82624754b3b8af5954e4c6a6d4d",
            "378ad860d7c041b6ac3efe5b340dc293",
            "7a6541f13feb46fba0d8e13d480505b4",
            "7af89b93b22f4ca7bcd2139c2871a77a",
            "b1d7d1683ba14187aceb757e21f9b013",
            "5cf36f2c1e524b738f6b8d3d648b708c",
            "3d36c5e754774faa9558f57d69b25ad3",
            "a738ee3f29d14cf3b806689e90e169e2"
          ]
        },
        "id": "AqTf5pw-aEg_",
        "outputId": "b5a7359f-ac5c-4698-a0e1-092228966701"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Converting model to GGUF format...\n",
            "Unsloth: Merging model weights to 16-bit format...\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `/tmp/unsloth_gguf_mjin8rn9`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `/tmp/unsloth_gguf_mjin8rn9`\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `/tmp/unsloth_gguf_mjin8rn9`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 165.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `/tmp/unsloth_gguf_mjin8rn9`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11155.06it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:27<00:00, 27.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/tmp/unsloth_gguf_mjin8rn9`\n",
            "Unsloth: Converting to GGUF format...\n",
            "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF f16 to ['q8_0'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: llama.cpp found in the system. Skipping installation.\n",
            "Unsloth: Preparing converter script...\n",
            "Unsloth: [1] Converting model into f16 GGUF format.\n",
            "This might take 3 minutes...\n",
            "Unsloth: Initial conversion completed! Files: ['gemma-3-270m-it.F16.gguf']\n",
            "Unsloth: [2] Converting GGUF f16 into q8_0. This might take 10 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Model files cleanup...\n",
            "Unsloth: All GGUF conversions completed successfully!\n",
            "Generated files: ['gemma-3-270m-it.Q8_0.gguf']\n",
            "Unsloth: example usage for text only LLMs: llama-cli --model gemma-3-270m-it.Q8_0.gguf -p \"why is the sky blue?\"\n",
            "Unsloth: Saved Ollama Modelfile to current directory\n",
            "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n",
            "Uploading gemma-3-270m-it.Q8_0.gguf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8b61e1f9105450faa474bac6cbbb69b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "908c70fd83414a659d9ee55900c0c828"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  gemma-3-270m-it.Q8_0.gguf   :  17%|#7        | 50.3MB /  292MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c25c3853e2e4e98a028d9e8af803021"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading config.json...\n",
            "Uploading Ollama Modelfile...\n",
            "Unsloth: Successfully uploaded GGUF to https://huggingface.co/mayurmadnani/gemma-3-270m-microfables\n",
            "Unsloth: Cleaning up temporary files...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mayurmadnani/gemma-3-270m-microfables'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "nudCsB4mleAs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.save_pretrained(f\"{LM_MODEL}-microfables-merged\")\n",
        "tokenizer.save_pretrained(f\"{LM_MODEL}-microfables-merged\")"
      ],
      "metadata": {
        "id": "RAs_1HxBn1Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc848325-ca56-4cb0-8003-4b574eba9555"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gemma-3-270m-it-microfables-merged/tokenizer_config.json',\n",
              " 'gemma-3-270m-it-microfables-merged/special_tokens_map.json',\n",
              " 'gemma-3-270m-it-microfables-merged/chat_template.jinja',\n",
              " 'gemma-3-270m-it-microfables-merged/tokenizer.model',\n",
              " 'gemma-3-270m-it-microfables-merged/added_tokens.json',\n",
              " 'gemma-3-270m-it-microfables-merged/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serving"
      ],
      "metadata": {
        "id": "wAdjsVh0RNUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "zgWjeXoXRLvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd88ed9-5cbb-492c-b9d6-3b6a12b44ae8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  38419      0 --:--:-- --:--:-- --:--:-- 38384\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama serve in the background\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'])\n",
        "\n",
        "# Give the processes a moment to start\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "B2QE7EePzDlr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama create gemma-3-270m-microfables -f ./Modelfile"
      ],
      "metadata": {
        "id": "UJzj8bDaUTw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dde2b89-f520-441e-cd5b-7783abf9d1fe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbA4blvEG2Ch",
        "outputId": "faa2741a-e88d-4e00-86e0-b53b1b00c04e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                               ID              SIZE      MODIFIED               \n",
            "gemma-3-270m-microfables:latest    2aac8b44638a    291 MB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434/api/generate -d '{ \\\n",
        "  \"model\": \"gemma-3-270m-microfables\", \\\n",
        "  \"prompt\": \"Write a short story about a brave knight.\", \\\n",
        "  \"stream\": false \\\n",
        "}'"
      ],
      "metadata": {
        "id": "mvmR9GBrVENy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284a617c-66e9-436e-ef80-be4d3173615a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma-3-270m-microfables\",\"created_at\":\"2025-10-31T18:05:23.502822654Z\",\"response\":\"A knight rode into battle, bold and strong. His armor gleamed under the sun. The king raised his sword, and the army cheered. The knight faced down an enemy king, his courage shining like the morning sun. His bravery saved lives and brought peace to all.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[105,2364,107,6974,496,2822,3925,1003,496,36711,52482,236761,106,107,105,4368,107,107,236776,52482,38965,1131,10041,236764,16627,532,3188,236761,4923,38119,20508,15608,1208,506,3768,236761,669,9615,8675,914,26114,236764,532,506,14093,120899,236761,669,52482,17175,1679,614,13550,9615,236764,914,23648,36489,1133,506,5597,3768,236761,4923,101666,10683,6176,532,6111,8118,531,784,236761],\"total_duration\":2695001066,\"load_duration\":2172676571,\"prompt_eval_count\":19,\"prompt_eval_duration\":14644714,\"eval_count\":56,\"eval_duration\":372159392}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama cp gemma-3-270m-microfables mayurmadnani/gemma-3-270m-microfables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN8iNTk5SFtR",
        "outputId": "0f58f340-5f68-4f99-9816-7712eb7286b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copied 'gemma-3-270m-microfables' to 'mayurmadnani/gemma-3-270m-microfables'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama push mayurmadnani/gemma-3-270m-microfables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgqcs9ooBvty",
        "outputId": "b470b975-dc56-42fb-9444-115a7e8181b5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            "You can find your model at:\n",
            "\n",
            "\thttps://ollama.com/mayurmadnani/gemma-3-270m-microfables\n"
          ]
        }
      ]
    }
  ]
}